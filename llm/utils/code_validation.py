"""
Code validation utilities for the LLM compiler.

This module provides tools for validating and fixing common issues
in PySpark code generated by the LLM.
"""

import re
from typing import List, Dict, Any


def validate_and_fix_code(code: str) -> str:
    """
    Validate and fix common code issues.
    
    Args:
        code: The code to validate and fix
        
    Returns:
        Fixed code
    """
    if not code:
        return code
    
    # Track if we made any fixes
    fixes_made = []
    
    # First fix: Ensure correct formatting for the UNCOMMENTED version of SparkSession
    # Since the executor will modify this, we need to make sure the uncommented version is also correct
    
    # Pattern for commented SparkSession builder with continuation format
    commented_builder_pattern = re.compile(r'# spark\s*=\s*SparkSession\.builder\s*\\?\s*\n(\s*)#\s*\.', re.MULTILINE)
    
    if commented_builder_pattern.search(code):
        # Fix the commented version first
        code = re.sub(
            commented_builder_pattern,
            lambda m: '# spark = SparkSession.builder\\\n' + m.group(1) + '#     .',
            code
        )
        fixes_made.append("Fixed commented SparkSession.builder pattern")
        print("✓ Fixed commented SparkSession.builder pattern")
    
    # Pattern for the SparkSession builder with continuation format
    spark_session_pattern = re.compile(r'(spark\s*=\s*SparkSession\.builder\s*\\?\s*\n)(\s*)\.', re.MULTILINE)
    
    if spark_session_pattern.search(code):
        # Fix the uncommented version
        code = re.sub(
            spark_session_pattern,
            lambda m: m.group(1) + '    .',
            code
        )
        fixes_made.append("Fixed uncommented SparkSession.builder pattern")
        print("✓ Fixed uncommented SparkSession.builder pattern")
    
    # Create a special direct replacement for SparkSession creation to avoid indentation issues
    spark_creation_pattern = re.compile(
        r'(?:# )?spark\s*=\s*SparkSession\.builder\s*\\?.*?\.getOrCreate\(\)',
        re.DOTALL
    )
    
    if spark_creation_pattern.search(code):
        # Replace with a properly formatted standard pattern 
        # This pattern works both commented and uncommented
        standard_builder = """# spark = SparkSession.builder\\
#     .appName("PySpark Transformation")\\
#     .getOrCreate()"""
        
        code = re.sub(
            spark_creation_pattern,
            standard_builder,
            code
        )
        fixes_made.append("Standardized SparkSession builder pattern")
        print("✓ Applied standard SparkSession builder pattern")
    
    # Fix common indentation errors with multi-line SparkSession creation
    lines = code.split('\n')
    fixed_lines = []
    
    # First pass: Fix SparkSession creation blocks
    in_spark_session = False
    builder_indent = 0
    session_start_line = -1
    continuation_line = False
    
    for i, line in enumerate(lines):
        # Detect SparkSession creation patterns
        if "SparkSession.builder" in line:
            in_spark_session = True
            session_start_line = i
            builder_indent = len(line) - len(line.lstrip())
            fixed_lines.append(line)
            continuation_line = line.strip().endswith("\\")
            print(f"✓ Found SparkSession.builder at line {i+1}, indent level: {builder_indent}")
            continue
            
        # Fix indentation in multi-line SparkSession creation
        if in_spark_session:
            strip_line = line.strip()
            
            # If this is a continuation of the builder pattern
            if strip_line.startswith('.') or continuation_line:
                # Apply proper indentation (builder_indent + 4 spaces)
                if not line.startswith(" " * (builder_indent + 4)) and strip_line:
                    fixed_line = " " * (builder_indent + 4) + strip_line
                    fixed_lines.append(fixed_line)
                    fixes_made.append(f"Fixed indentation in SparkSession builder at line {i+1}")
                    print(f"✓ Fixed line {i+1}: '{line}' -> '{fixed_line}'")
                else:
                    fixed_lines.append(line)
            
                continuation_line = strip_line.endswith("\\")
                
                # Check if we're done with the SparkSession creation
                if strip_line.endswith(".getOrCreate()") or "getOrCreate()" in strip_line:
                    in_spark_session = False
                    print(f"✓ Completed SparkSession builder block at line {i+1}")
            else:
                # We've exited the SparkSession creation
                in_spark_session = False
                fixed_lines.append(line)
        else:
            fixed_lines.append(line)
    
    # Second pass: Fix method chaining patterns throughout the code
    fixed_code = "\n".join(fixed_lines)
    lines = fixed_code.split('\n')
    fixed_lines = []
    
    # Track backslash continuation to maintain state between lines
    continuation_active = False
    continuation_indent = 0
    
    for i, line in enumerate(lines):
        strip_line = line.strip()
        current_indent = len(line) - len(line.lstrip())
        
        # Handle continuation from previous line
        if continuation_active and strip_line and not strip_line.startswith('#'):
            # This line should be indented to match continuation_indent
            if current_indent != continuation_indent and not strip_line.startswith('.'):
                fixed_line = " " * continuation_indent + strip_line
                fixed_lines.append(fixed_line)
                fixes_made.append(f"Fixed continuation indentation at line {i+1}")
                print(f"✓ Fixed continuation at line {i+1}: '{line}' -> '{fixed_line}'")
            else:
                fixed_lines.append(line)
        
            # Check if continuation continues with this line
            continuation_active = strip_line.endswith("\\")
        
        # If this line starts with a dot (method chaining), it should align with previous line
        elif strip_line.startswith('.') and i > 0:
            prev_line = lines[i-1].rstrip()
            prev_indent = len(lines[i-1]) - len(lines[i-1].lstrip())
            
            if prev_line.endswith("\\"):
                # Previous line has continuation - match its indent + 4
                if current_indent != prev_indent + 4:
                    fixed_line = " " * (prev_indent + 4) + strip_line
                    fixed_lines.append(fixed_line)
                    fixes_made.append(f"Fixed dot-method indentation at line {i+1}")
                    print(f"✓ Fixed dot-method indent at line {i+1}: '{line}' -> '{fixed_line}'")
                else:
                    fixed_lines.append(line)
            else:
                # Regular method chaining - match previous indent + 4 or + 2
                if not line.startswith(' ' * (prev_indent + 4)) and not line.startswith(' ' * (prev_indent + 2)):
                    fixed_line = ' ' * (prev_indent + 4) + strip_line
                    fixed_lines.append(fixed_line)
                    fixes_made.append(f"Fixed dot-method indentation at line {i+1}")
                    print(f"✓ Fixed method chain at line {i+1}: '{line}' -> '{fixed_line}'")
                else:
                    fixed_lines.append(line)
        else:
            fixed_lines.append(line)
        
        # Start tracking continuation if this line ends with backslash
        if strip_line.endswith("\\"):
            continuation_active = True
            continuation_indent = current_indent + 4
        else:
            continuation_active = False
    
    fixed_code = "\n".join(fixed_lines)
    
    # Fix common column reference patterns for joins to prevent ambiguity
    # Change: df.join(other_df, "id") -> df.join(other_df, df["id"] == other_df["id"])
    join_pattern = r'\.join\s*\(\s*([a-zA-Z_][a-zA-Z0-9_]*)\s*,\s*["\']([a-zA-Z_][a-zA-Z0-9_]*)["\']'
    if re.search(join_pattern, fixed_code):
        fixed_code = re.sub(
            join_pattern,
            lambda m: f'.join({m.group(1)}, F.col("{m.group(2)}") == {m.group(1)}["{m.group(2)}"]',
            fixed_code
        )
        fixes_made.append("Fixed ambiguous join conditions")
        print("✓ Fixed ambiguous join conditions")
    
    # Sometimes column references aren't qualified in where clauses, filter, etc.
    # Try to fix those by prepending "df." to ensure proper qualification
    # The regex catches common filter/where patterns with bare column references
    condition_pattern = r'(\.(filter|where)\s*\(\s*)([a-zA-Z_][a-zA-Z0-9_]*)\s*(==|!=|>|<|>=|<=)'
    if re.search(condition_pattern, fixed_code):
        fixed_code = re.sub(
            condition_pattern,
            lambda m: f'{m.group(1)}F.col("{m.group(3)}") {m.group(4)}',
            fixed_code
        )
        fixes_made.append("Fixed unqualified column references in conditions")
        print("✓ Fixed unqualified column references")
    
    # Add a helper function to standardize join conditions to prevent ambiguity
    def standardize_join_conditions(match):
        full_join = match.group(0)
        df_name = match.group(1)
        other_df = match.group(2)
        on_condition = match.group(3).strip()
        
        # If the join already uses dataframe["column"] == other["column"] format, keep it
        if "==" in on_condition:
            return full_join
            
        # Strip quotes if present
        on_condition = on_condition.strip("'\"")
        
        # Simple case - single column name
        if on_condition.count(',') == 0 and not on_condition.startswith('['):
            return f"{df_name}.join({other_df}, {df_name}[\"{on_condition}\"] == {other_df}[\"{on_condition}\"])"
        
        # Multiple columns case (comma-separated)
        if ',' in on_condition:
            columns = [c.strip().strip("'\"") for c in on_condition.split(',')]
            conditions = " & ".join([f"{df_name}[\"{col}\"] == {other_df}[\"{col}\"]" for col in columns])
            return f"{df_name}.join({other_df}, {conditions})"
            
        # If it's already a complex condition, leave it unchanged
        return full_join
    
    # Fix simple join conditions to prevent ambiguous column references
    # Matches patterns like: df.join(other_df, "column_name") or df.join(other_df, ["col1", "col2"])
    join_pattern = re.compile(r'(\w+)\.join\((\w+),\s*([^)]+)\)')
    code = join_pattern.sub(standardize_join_conditions, code)
    
    # Add explicit error-pattern fixes
    # Fix ambiguous column references in join conditions
    ambiguous_ref_pattern = re.compile(r'(\w+)\.(\w+)\s*==\s*(\w+)\.(\w+)')
    def fix_ambiguous_refs(match):
        left_df, left_col, right_df, right_col = match.groups()
        return f"{left_df}[\"{left_col}\"] == {right_df}[\"{right_col}\"]"
    
    code = ambiguous_ref_pattern.sub(fix_ambiguous_refs, code)
    
    # Find dataframe alias uses and standardize column references
    # This will replace df.column with df["column"] to avoid ambiguity
    df_column_pattern = re.compile(r'(\w+)\.([a-zA-Z]\w*(?:\([^)]*\))?)')
    def replace_column_refs(match):
        df_name, col_ref = match.groups()
        
        # Skip if the column reference is actually a method call like filter, select, etc.
        spark_methods = ['join', 'select', 'filter', 'where', 'groupBy', 'agg', 'orderBy', 
                         'withColumn', 'drop', 'limit', 'count', 'show', 'head', 'take',
                         'distinct', 'dropDuplicates', 'withColumnRenamed', 'createOrReplaceTempView',
                         'collect', 'toPandas', 'printSchema', 'write', 'unionByName', 'union',
                         'intersect', 'subtract']
        
        # Don't modify method calls
        if col_ref in spark_methods or '(' in col_ref:
            return match.group(0)
            
        # Convert df.column to df["column"] syntax
        return f'{df_name}["{col_ref}"]'
        
    # Apply the column reference fix
    # Skip this fix for session objects like spark, sc, etc.
    non_df_objects = ['spark', 'sc', 'SparkSession', 'functions', 'F', 'Window', 'types', 
                      'org', 'jdbc', 'http', 'https', 'com', 'net', 'io', 'java', 'scala',
                      'postgresql', 'mysql', 'sqlserver', 'oracle', 'Driver', 'Connection']
    code_lines = code.split('\n')
    for i, line in enumerate(code_lines):
        # Skip fixing lines that mention non-dataframe objects
        if any(obj in line for obj in non_df_objects):
            continue
            
        # Skip string literals that look like JDBC URLs or package names
        if '"driver"' in line.lower() or '"url"' in line.lower() or 'import' in line.lower():
            continue
            
        # Skip lines that are inside string literals containing dots
        if ('"' in line and "." in line and line.count('"') >= 2) or \
           ("'" in line and "." in line and line.count("'") >= 2):
            continue
            
        code_lines[i] = df_column_pattern.sub(replace_column_refs, line)
    code = '\n'.join(code_lines)

    # Fix alias collisions in the code
    def fix_alias_collisions(code):
        # Track DataFrames and their aliases
        df_aliases = {}  # Maps DataFrame name to its alias
        alias_dfs = {}   # Maps alias to DataFrame name
        
        # Define patterns for alias definition and usage
        alias_def_pattern = re.compile(r'(\w+)\s+as\s+(\w+)', re.IGNORECASE)
        alias_assign_pattern = re.compile(r'(\w+)\s*=\s*(\w+)\.alias\([\'"](\w+)[\'"]\)')
        
        lines = code.split('\n')
        modified = False
        
        # First pass: collect all aliases
        for i, line in enumerate(lines):
            # Check for SQL-style aliases: df as alias
            for match in alias_def_pattern.finditer(line):
                df_name, alias = match.groups()
                if alias in alias_dfs and alias_dfs[alias] != df_name:
                    # Alias collision detected
                    modified = True
                    new_alias = f"{alias}_{df_name}"
                    # Replace in this line
                    lines[i] = line.replace(f"{df_name} as {alias}", f"{df_name} as {new_alias}")
                    df_aliases[df_name] = new_alias
                    alias_dfs[new_alias] = df_name
                else:
                    df_aliases[df_name] = alias
                    alias_dfs[alias] = df_name
            
            # Check for PySpark-style aliases: new_df = df.alias("alias")
            for match in alias_assign_pattern.finditer(line):
                new_df, orig_df, alias = match.groups()
                if alias in alias_dfs and alias_dfs[alias] != new_df:
                    # Alias collision detected
                    modified = True
                    new_alias = f"{alias}_{new_df}"
                    # Replace in this line
                    lines[i] = line.replace(f'alias("{alias}")', f'alias("{new_alias}")')
                    df_aliases[new_df] = new_alias
                    alias_dfs[new_alias] = new_df
                else:
                    df_aliases[new_df] = alias
                    alias_dfs[alias] = new_df
        
        # Second pass: update all alias references if we found collisions
        if modified:
            for i, line in enumerate(lines):
                for df_name, alias in df_aliases.items():
                    # Replace references to the alias in the code
                    # This is a simplified approach - might need improvement for complex cases
                    if alias.startswith(df_name):
                        # This was a renamed alias, update references
                        old_alias = alias.split('_')[0]
                        # Replace standalone old alias with new alias (with word boundaries)
                        lines[i] = re.sub(r'\b' + old_alias + r'\b', alias, lines[i])
        
        return '\n'.join(lines)
    
    # Apply alias collision fixing
    fixed_code = fix_alias_collisions(code)
    if fixed_code != code:
        code = fixed_code
        fixes_made.append("Fixed alias collisions")
        print("✓ Fixed alias collisions")

    # Add alias uniqueness to prompt for future code generation
    def generate_prompt_with_fixes():
        return """
        # Code Quality Guidelines
        - Use explicit aliases for all DataFrame operations (e.g., df.alias("unique_name"))
        - Ensure all aliases are unique across the query
        - Use explicit join conditions with fully qualified column names:
          GOOD: df1.join(df2, df1["id"] == df2["id"])
          AVOID: df1.join(df2, "id")
        - Use bracket notation for column references:
          GOOD: df["column_name"]
          AVOID: df.column_name
        """
    
    code_quality_prompt = generate_prompt_with_fixes()
    
    if fixes_made:
        print(f"Code fixes applied: {', '.join(fixes_made)}")
    
    return code 